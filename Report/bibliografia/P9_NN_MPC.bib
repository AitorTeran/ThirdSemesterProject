Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@misc{Abiodun2018,
abstract = {This is a survey of neural network applications in the real-world scenario. It provides a taxonomy of artificial neural networks (ANNs) and furnish the reader with knowledge of current and emerging trends in ANN applications research and area of focus for researchers. Additionally, the study presents ANN application challenges, contributions, compare performances and critiques methods. The study covers many applications of ANN techniques in various disciplines which include computing, science, engineering, medicine, environmental, agriculture, mining, technology, climate, business, arts, and nanotechnology, etc. The study assesses ANN contributions, compare performances and critiques methods. The study found that neural-network models such as feedforward and feedback propagation artificial neural networks are performing better in its application to human problems. Therefore, we proposed feedforward and feedback propagation ANN models for research focus based on data analysis factors like accuracy, processing speed, latency, fault tolerance, volume, scalability, convergence, and performance. Moreover, we recommend that instead of applying a single method, future research can focus on combining ANN models into one network-wide application.},
author = {Abiodun, Oludare Isaac and Jantan, Aman and Omolara, Abiodun Esther and Dada, Kemi Victoria and Mohamed, Nachaat Abd Elatif and Arshad, Humaira},
booktitle = {Heliyon},
doi = {10.1016/j.heliyon.2018.e00938},
issn = {24058440},
keywords = {Computer science},
month = {nov},
number = {11},
publisher = {Elsevier Ltd},
title = {{State-of-the-art in artificial neural network applications: A survey}},
volume = {4},
year = {2018}
}
@techreport{Glorot2011,
abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hy-perbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
file = {::},
title = {{Deep Sparse Rectifier Neural Networks}},
year = {2011}
}
@article{Dragicevic2018,
abstract = {This paper proposes the application of a finite control set model predictive control (FCS-MPC) strategy in standalone ac microgrids (MGs). AC MGs are usually built from two or more voltage source converters (VSCs) which have the capability of regulating the voltage at the point of common coupling, while sharing the load power at the same time. Those functionalities are conventionally achieved by hierarchical linear control loops. However, they present severe limitations in terms of slow transient response and high sensitivity to parameter variations. This paper aims to mitigate these problems by first introducing an improvement of the FCS-MPC strategy for a single VSC that is based on explicit tracking of derivative of the voltage reference trajectory. Using only a single step prediction horizon, the proposed strategy exhibits very low computational expense, but provides steady-state performance comparable to carrier-based sinusoidal PWM, while its transient response and robustness to parameter variation is far superior to hierarchical linear control. These benefits are exploited in a general ac MG setting where a methodology for paralleling multiple FCS-MPC regulated VSCs is described. Such an MG is characterized by rapid transient response, inherent stability in all operating conditions, and fully decentralized operation of individual VSCs. These findings have been validated through comprehensive simulation and experimental verification.},
author = {Dragicevic, Tomislav},
doi = {10.1109/TPEL.2017.2744986},
file = {::},
issn = {08858993},
journal = {IEEE Transactions on Power Electronics},
keywords = {AC microgrid (MG),finite control set (FCS),model predictive control (MPC),voltage source converter (VSC)},
number = {7},
pages = {6304--6317},
title = {{Model Predictive Control of Power Converters for Robust and Fast Operation of AC Microgrids}},
volume = {33},
year = {2018}
}
@book{Kumar2014,
author = {Rashid, Muhammad and Kumar, Narendra and Kulkarni, Ashish},
edition = {4th},
file = {::},
isbn = {978-0-273-76908-8},
pages = {1027},
publisher = {Pearson},
title = {{Power Electronics Devices, Circuits and Applications}},
year = {2014}
}
@misc{switchcraft,
author = {Soldbakken, Yingve},
title = {{Space Vector PWM Intro}},
url = {https://www.switchcraft.org/learning/2017/3/15/space-vector-pwm-intro},
urldate = {2019-12-04},
year = {2017}
}
@techreport{pere_proy,
author = {Luntrasu, Flavius Alexandru and Valeur, Frederik Sabro and Zubavicius, Gintaras and Izquierdo, Pere and Kerekes, Tamas},
file = {::},
title = {{Modelling and Control of a Three-Switch , Three-Phase DC to AC Converter}},
year = {2019}
}
@techreport{Meijering2002,
abstract = {This paper presents a chronological overview of the developments in interpolation theory, from the earliest times to the present date. It brings out the connections between the results obtained in different ages, thereby putting the techniques currently used in signal and image processing into historical perspective. A summary of the insights and recommendations that follow from relatively recent theoretical as well as experimental studies concludes the presentation .},
author = {Meijering, Erik},
file = {::},
keywords = {Approximation,convolution-based interpolation,history,image processing,polynomial interpolation,signal pro-cessing,splines},
title = {{A Chronology of Interpolation: From Ancient Astronomy to Modern Signal and Image Processing}},
year = {2002}
}
@article{Dragicevic2019,
abstract = {This paper proposes the use of an artificial neural network (ANN) for solving one of the ongoing research challenges in finite set-model predictive control (FS-MPC) of power electronics converters, i.e., the automated selection of weighting factors in cost function. The first step in this approach is to simulate a detailed converter circuit model or run experiments numerous times using different weighting factor combinations. The key performance metrics [e.g., average switching frequency (fsw) of the converter, total harmonic distortion, etc.] are extracted from each simulation. This data is then used to train the ANN, which serves as a surrogate model of the converter that can provide fast and accurate estimates of the performance metrics for any weighting factor combination. Consequently, any arbitrary user-defined fitness function that combines the output metrics can be defined and the weighting factor combinations that optimize the given function can be explicitly found. The proposed methodology was verified on a practical weighting factor design problem in FS-MPC regulated voltage source converter for uninterruptible power supply system. Designed weighting factors for two exemplary fitness functions turned out to be robust to load variations and to yield close to expected performance when applied both to detailed simulation model (less than 3{\%} error) and to experimental test bed (less than 10{\%} error).},
author = {Dragi{\v{c}}evi{\'{c}}, Tomislav and Novak, Mateja},
doi = {10.1109/TIE.2018.2875660},
file = {::},
issn = {02780046},
journal = {IEEE Transactions on Industrial Electronics},
keywords = {Artificial neural network (ANN),finite set-model predictive control (FS-MPC),voltage source converter (VSC),weighing factor design},
number = {11},
pages = {8870--8880},
title = {{Weighting Factor Design in Model Predictive Control of Power Electronic Converters: An Artificial Neural Network Approach}},
volume = {66},
year = {2019}
}
@techreport{Leshno1993,
abstract = {Several researchers characterized the activation fimction under which multilayer feedforward networks can act as universal approximators. We show that most of all the characterizations that were reported thus far in the literature are special cases of the following general result: A standard multilayer feedforward network with a locally bounded piecewise continuous activation fimction can approximate an3, continuous function to any degree of accuracy if and only if the network's activation function is not a polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem does not hold.},
author = {Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
booktitle = {Neural Networks},
file = {::},
keywords = {) approximation,Activation functions,LP(t,Multilayer feedforward networks,Role of threshold,Universal approximation capabilities},
pages = {861--867},
title = {{Multilayer Feedforward Networks With a Nonpolynomial Activation Function Can Approximate Any Function}},
volume = {6},
year = {1993}
}
@article{Imitation_learning,
author = {Electronics, Industrial},
file = {:C$\backslash$:/Users/perei/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Electronics - Unknown - Supervised imitation learning of model predictive control systems for power electronics.pdf:pdf},
keywords = {are any of authors,ieee,learning control systems,neural networks,predictive control},
title = {{Supervised imitation learning of model predictive control systems for power electronics}}
}
@book{IanGoodfellowYoshuaBengio2017,
abstract = {Deep Learning book},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{Ian Goodfellow, Yoshua Bengio}, Aaron Courville},
booktitle = {MIT Press},
doi = {10.1016/B978-0-12-391420-0.09987-X},
eprint = {arXiv:1011.1669v3},
file = {:D$\backslash$:/Documents/AAU/3rd Semester/Project/Papers/Ian Goodfellow, Yoshua Bengio, Aaron Courville - Deep Learning (2017, MIT).pdf:pdf},
isbn = {3540620583, 9783540620587},
issn = {1432122X},
number = {7553},
pages = {785},
pmid = {21728107},
title = {{Deep Learning}},
volume = {521},
year = {2017}
}
@inproceedings{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
booktitle = {3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings},
eprint = {1412.6980},
file = {::},
publisher = {International Conference on Learning Representations, ICLR},
title = {{Adam: A method for stochastic optimization}},
year = {2015}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
issn = {00280836},
journal = {Nature},
number = {6088},
pages = {533--536},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
